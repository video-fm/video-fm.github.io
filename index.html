<!DOCTYPE html><html><head><meta charSet="utf-8" data-next-head=""/><meta name="viewport" content="width=device-width, initial-scale=1" data-next-head=""/><title data-next-head="">VINE: Video Intelligence Foundation Model</title><meta name="description" content="VINE: A Foundation Model for Video Understanding" data-next-head=""/><link rel="preload" href="/laser-website/_next/static/css/0ce439ca0032a9d5.css" as="style"/><link rel="stylesheet" href="/laser-website/_next/static/css/0ce439ca0032a9d5.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" noModule="" src="/laser-website/_next/static/chunks/polyfills-42372ed130431b0a.js"></script><script src="/laser-website/_next/static/chunks/webpack-05d528e5659a88ea.js" defer=""></script><script src="/laser-website/_next/static/chunks/framework-2f335d22a7318891.js" defer=""></script><script src="/laser-website/_next/static/chunks/main-5a50fb2be1393fea.js" defer=""></script><script src="/laser-website/_next/static/chunks/pages/_app-d286934c02fe3d41.js" defer=""></script><script src="/laser-website/_next/static/chunks/pages/index-2a4fe176484a1f13.js" defer=""></script><script src="/laser-website/_next/static/rUz_Yh65ZemNDVJCWT-ph/_buildManifest.js" defer=""></script><script src="/laser-website/_next/static/rUz_Yh65ZemNDVJCWT-ph/_ssgManifest.js" defer=""></script></head><body><link rel="preload" as="image" href="/images/logo.png"/><link rel="preload" as="image" href="/images/penn_logo.png"/><div id="__next"><nav class="nav"><div class="nav-container"><div class="nav-brand"><img src="/images/logo.png" alt="VINE Logo" class="nav-logo"/><div class="nav-text"><span class="brand-text">VINE</span><span class="brand-subtitle">A Foundation Model for Video Understanding</span></div></div></div></nav><section class="hero"><div class="hero-background"><div class="hero-content"><div class="hero-text"><div class="hero-title-container"><h1 class="hero-title">VINE: A Foundation Model for Video Understanding</h1><div class="hero-logo"><img src="/images/logo.png" alt="VINE Logo" class="hero-logo-image"/></div></div><div class="hero-quote">&quot;If a picture is worth 1000 words, how many is a video worth?&quot;</div><p class="hero-description">Video understanding is the future. There is a wealth of data locked inside video that traditional methods cannot unlock. Our foundation model helps extract meaningful insights from real-time video data, with future applications in healthcare monitoring, robotic perception, and security analysis.</p><p class="hero-description">VINE is a foundation model specifically designed for real-time video understanding, providing unprecedented capabilities for extracting spatio-temporal scene graphs from video data.</p><div class="hero-actions"><a href="#" class="demo-button"><span class="button-icon">â–¶</span>Try the demo</a></div></div></div></div><div class="hero-videos"><div class="video-grid-hero video-grid-two"><div class="video-item"><video src="/annotated_videos/2.mp4" class="hero-video" autoPlay="" muted="" loop="" playsInline=""></video></div><div class="video-item"><video src="/annotated_videos/4.mp4" class="hero-video" autoPlay="" muted="" loop="" playsInline=""></video></div></div><div class="hero-video-caption">VINE running on videos, outputting the scene graph for each frame</div></div></section><main class="main-content"><section class="section"><div class="section-container"><h2 class="section-title">Modularity</h2><p class="section-description">VINE&#x27;s architecture is designed with flexibility at its core, allowing seamless integration with various state-of-the-art segmentation models. VINE can leverage Grounding Dino, YOLO, or SAM for highly accurate zero-shot segmentationâ€”all without modifying the core framework. This modular design means you can choose the segmentation backend that best fits your use case: prioritize speed with YOLO for live applications, accuracy with SAM for detailed analysis, or flexibility with DINO for open world detection. The segmentation module feeds its masks or bounding boxes into our fine-tuned CLIP model, which then generates the spatio-temporal scene graph. This plug-and-play approach ensures VINE stays current with advances in computer vision while maintaining consistent downstream performance.</p><div class="content-grid"><div class="video-showcase"><video src="/annotated_videos/1.mp4" class="showcase-video" autoPlay="" muted="" loop="" playsInline=""></video><div class="video-caption">VINE run with SAM with masks</div></div><div class="video-showcase"><video src="/annotated_videos/7.mp4" class="showcase-video" autoPlay="" muted="" loop="" playsInline=""></video><div class="video-caption">VINE run with Grounding DINO with bounding boxes</div></div></div></div></section><section class="section section-gray"><div class="section-container"><h2 class="section-title">Efficiency</h2><p class="section-description">VINE is engineered for speed and accessibility, running smoothly on everything from consumer CPUs to high-end GPUs and cloud TPUs. Unlike heavyweight video understanding models that demand specialized hardware, VINE&#x27;s efficient late fusion architecture keeps computational requirements minimal while maintaining real-time performance. The framework is compatible with both PyTorch and JAX, allowing developers to leverage their preferred ecosystem and hardware acceleration.</p><div class="performance-tables"><div class="table-container"><h3 class="table-title">Performance Benchmarks</h3><div class="table-placeholder"><div class="placeholder-content"><div class="placeholder-text">Performance comparison tables will be displayed here</div><div class="placeholder-subtext">Hardware compatibility and inference speed metrics</div></div></div></div><div class="table-container"><h3 class="table-title">Hardware Compatibility</h3><div class="table-placeholder"><div class="placeholder-content"><div class="placeholder-text">Hardware support matrix will be displayed here</div><div class="placeholder-subtext">CPU, GPU, and TPU performance metrics</div></div></div></div></div></div></section><section class="section"><div class="section-container"><h2 class="section-title">Zero-shot Generalizability</h2><p class="section-description">VINE has learned a general notion of what scene graphs are -- this understanding enables zero-shot generalization to unfamiliar objects and actions without requiring additional training. We show VINE on various action localization tasks without any further finetuning.</p><div class="zero-shot-videos"><div class="video-grid-four"><div class="video-item"><video src="/annotated_videos/8.mp4" class="zero-shot-video" autoPlay="" muted="" loop="" playsInline=""></video></div><div class="video-item"><video src="/annotated_videos/9.mp4" class="zero-shot-video" autoPlay="" muted="" loop="" playsInline=""></video></div><div class="video-item"><video src="/annotated_videos/10.mp4" class="zero-shot-video" autoPlay="" muted="" loop="" playsInline=""></video></div><div class="video-item"><video src="/annotated_videos/11.mp4" class="zero-shot-video" autoPlay="" muted="" loop="" playsInline=""></video></div></div><div class="video-caption">Zero-shot action localization across diverse scenarios without additional training</div></div></div></section><section class="section section-gray"><div class="section-container"><h2 class="section-title">Promptability and Finetunability</h2><p class="section-description">VINE&#x27;s foundation architecture enables powerful downstream adaptation through both prompting and fine-tuning strategies. The model can be dynamically prompted to focus on specific objects and relationships, returning probabilistic confidence scores for detected entities and their interactions.</p><div class="promptability-demo"><div class="demo-container"><div class="input-section"><div class="input-group"><label class="input-label">Objects to prompt:</label><div class="input-controls"><select class="objects-dropdown"><option value="[&quot;person&quot;,&quot;sofa&quot;]" selected="">person, sofa</option><option value="[&quot;sofa&quot;,&quot;dog&quot;]">sofa, dog</option><option value="[&quot;person&quot;,&quot;dog&quot;]">person, dog</option></select><button class="play-button"><span class="play-icon">â–¶</span></button></div></div></div><div class="demo-output"><div class="demo-placeholder"><div class="placeholder-icon">ðŸŽ¯</div><p>Select objects and click play to see VINE in action</p></div></div></div></div><div class="promptability-content"><div class="content-grid"><div class="feature-block"><h3 class="feature-title">Probabilistic Prompting</h3><p class="feature-description">VINE operates probabilistically, allowing you to prompt for specific objects, actions, or relationships and receive confidence scores for all detected entities. Rather than binary detection, VINE provides  probability distributions across the entire scene graph, enabling fine-grained control over what the model focuses on during inference.</p></div><div class="feature-block"><h3 class="feature-title">Fine-tuning &amp; Adaptation</h3><p class="feature-description">Beyond prompting, VINE&#x27;s foundation model can be efficiently fine-tuned for specialized downstream tasks. The modular architecture enables task-specific adaptation through either full finetuning or parameter-efficient techniques while preserving the core video understanding capabilities, making it suitable for domain-specific applications across industries.</p></div></div><div class="performance-results"><h3 class="results-title">Finetuned Action Recognition Performance</h3><p class="results-description">VINE demonstrates strong performance on action recognition across different training scenarios on ActivityNet. We compare against state-of-the-art action recognition models including BIKE, Text4Vis, ResT, and E2E, showing competitive zero-shot and finetuned capabilities.</p><div class="performance-table-container"><table class="performance-table"><thead><tr><th>Category</th><th>Model</th><th>ActivityNet Accuracy (%)</th></tr></thead><tbody><tr class="category-row"><td rowSpan="6" class="category-cell">Zero-shot</td><td>SGClip</td><td class="accuracy-cell">76.34</td></tr><tr><td>CLIP</td><td class="accuracy-cell">74.37</td></tr><tr><td>BIKE</td><td class="accuracy-cell highlight">80.00</td></tr><tr><td>Text4vis</td><td class="accuracy-cell">77.40</td></tr><tr><td>ResT</td><td class="accuracy-cell">26.30</td></tr><tr><td>E2E</td><td class="accuracy-cell">20.00</td></tr><tr class="category-row"><td rowSpan="2" class="category-cell">Few-shot (1%)</td><td>SGClip</td><td class="accuracy-cell highlight">80.10</td></tr><tr><td>CLIP</td><td class="accuracy-cell">78.79</td></tr><tr class="category-row"><td rowSpan="2" class="category-cell">Few-shot (5%)</td><td>SGClip</td><td class="accuracy-cell highlight">86.05</td></tr><tr><td>CLIP</td><td class="accuracy-cell">80.02</td></tr></tbody></table></div><p class="table-caption">Action recognition accuracy on ActivityNet for zero-shot and few-shot (finetuned on 1% and 5% of the data) models. Zero-shot baselines include state-of-the-art action recognition models (BIKE, Text4Vis, ResT, E2E) and our models evaluated without training.</p></div></div></div></section><section class="section"><div class="section-container"><h2 class="section-title">Dataset</h2><div class="dataset-header"><h3 class="dataset-name">ESCA-Video-87K</h3><p class="dataset-tagline">A new benchmark for video understanding</p></div><div class="dataset-layout"><div class="dataset-content"><div class="dataset-description"><p class="dataset-overview"><strong>87,045 video clips</strong> curated and annotated to push the boundaries of video understanding. Each clip is paired with rich, natural language captions crafted by GPT-4.</p><p class="dataset-details">Our dataset uses precise object traces, dynamically segmented using Grounding DINO and SAM2. With programmatic specifications built in linear temporal logic, every clip becomes a structured video you can track, query, and reason aboutâ€”frame by frame.</p></div></div><div class="dataset-video-showcase"><video src="/annotated_videos/dataset.mp4" class="dataset-video" autoPlay="" muted="" loop="" playsInline=""></video><div class="video-caption">ESCA-Video-87K dataset samples</div></div></div><div class="dataset-stats"><div class="stat-item"><div class="stat-number">87K+</div><div class="stat-label">Video Clips</div></div><div class="stat-item"><div class="stat-number">100</div><div class="stat-label">Trajectories per vid</div></div><div class="stat-item"><div class="stat-number">500k+</div><div class="stat-label">Masks</div></div><div class="stat-item"><div class="stat-number">GPT-4</div><div class="stat-label">Captions</div></div></div></div></section><section class="section team"><div class="section-container"><h2 class="section-title">Team</h2><div class="team-grid"><div class="team-category"><h3 class="team-title">Core Contributors</h3><div class="team-members"><div class="team-member"><div class="member-name">Jiani Huang</div><div class="member-affiliation">University of Pennsylvania</div></div><div class="team-member"><div class="member-name">Amish Sethi</div><div class="member-affiliation">University of Pennsylvania</div></div><div class="team-member"><div class="member-name">Matthew Kuo</div><div class="member-affiliation">University of Pennsylvania</div></div></div></div><div class="team-category"><h3 class="team-title">Collaborators</h3><div class="team-members"><div class="team-member"><div class="member-name">Ziyang Li</div><div class="member-affiliation">University of Pennsylvania</div></div><div class="team-member"><div class="member-name">Mayank Keoliya</div><div class="member-affiliation">University of Pennsylvania</div></div><div class="team-member"><div class="member-name">Neelay Velingker</div><div class="member-affiliation">University of Pennsylvania</div></div></div></div><div class="team-category"><h3 class="team-title">Faculty</h3><div class="team-members"><div class="team-member"><div class="member-name">Mayur Naik</div><div class="member-affiliation">University of Pennsylvania</div></div><div class="team-member"><div class="member-name">Sernam Lim</div><div class="member-affiliation">University of Central Florida</div></div></div></div></div><div class="penn-logo-section"><div class="penn-logo"><img src="/images/penn_logo.png" alt="University of Pennsylvania Logo" class="penn-logo-image"/><div class="penn-text">University of Pennsylvania</div></div></div></div></section></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{}},"page":"/","query":{},"buildId":"rUz_Yh65ZemNDVJCWT-ph","assetPrefix":"/laser-website","nextExport":true,"autoExport":true,"isFallback":false,"scriptLoader":[]}</script></body></html>